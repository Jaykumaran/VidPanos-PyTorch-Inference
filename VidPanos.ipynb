{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c31d3d7b-3918-4fbb-879b-43117e09ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e3b7ef2-7a9a-476c-abe4-05478a5b740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video(path: str):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    ret = True\n",
    "    while ret:\n",
    "        ret, img = cap.read()\n",
    "        if ret:\n",
    "            frames.append(img)\n",
    "    video = np.stack(frames, axis = 0) #(T,H,W,C) Total frames, Height, Width, Channels\n",
    "    video_tensor = torch.tensor(np.transpose(video, (3,0,1,2))[np.newaxis, ...]).detach().to('cuda').float()\n",
    "    return video_tensor\n",
    "\n",
    "#TypeError: can't convert np.ndarray of type numpy.object_, indicates that the video array is being stored as a numpy.object_ type, which typically happens when the array contains elements of varying types or shapes. PyTorch’s torch.tensor() function expects a numpy array with a specific, uniform data type (such as float32, int32, etc.), and numpy.object_ isn't one of the supported types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5c0215b-f2b3-49d3-9f84-f58610b2b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_video_from_frames(path: str, num_frames: int):\n",
    "    frames = []\n",
    "    for i in range(num_frames):\n",
    "        full_path = f'{path}/frame_{i:04d}.jpg'\n",
    "        im = cv2.imread(full_path)\n",
    "        \n",
    "        # Ensure the image is in RGB format\n",
    "        if im is None:\n",
    "            raise FileNotFoundError(f\"Frame {full_path} not found.\")\n",
    "        if len(im.shape) == 2:  # If the image is grayscale\n",
    "            im = cv2.cvtColor(im, cv2.COLOR_GRAY2RGB)  # Convert grayscale to RGB\n",
    "        \n",
    "        frames.append(im)\n",
    "    \n",
    "    # Stack frames into a numpy array with shape (T, H, W, C)\n",
    "    video = np.stack(frames, axis=0)\n",
    "    \n",
    "    # Transpose to (C, T, H, W) and add batch dimension\n",
    "    video_tensor = torch.tensor(np.transpose(video, (3, 0, 1, 2))[np.newaxis, ...]).detach().to('cuda').float()\n",
    "    \n",
    "    return video_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b44054b9-993f-458d-a77e-81e7f6e927f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_video(path: str):\n",
    "#   frames = []\n",
    "#   cap = cv2.VideoCapture(path)\n",
    "#   ret = True\n",
    "#   while ret:\n",
    "#       ret, img = cap.read() # read one frame from the 'capture' object; img is (H, W, C)\n",
    "#       if ret:\n",
    "#           frames.append(img)\n",
    "#   video = np.stack(frames, axis=0) # dimensions (T, H, W, C)\n",
    "#   return torch.tensor(np.transpose(video, (3,0,1,2))[np.newaxis, ...]).detach().to('cuda').float()\n",
    "\n",
    "# def read_video_from_frames(path: str, num_frames: int):\n",
    "#   frames = []\n",
    "#   for i in range(num_frames):\n",
    "#     full_path = f'{path}/{i:04d}.png'\n",
    "#     im = cv2.imread(full_path)\n",
    "#     frames.append(im)\n",
    "#   video = np.stack(frames, axis=0) # dimensions (T, H, W, C)\n",
    "#   # return video\n",
    "#   return torch.tensor(np.transpose(video, (3,0,1,2))[np.newaxis, ...]).detach().to('cuda').float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1604056-7698-47cc-9949-a30068bffd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpvaegan_code.evaluate_sifid_our_samples import (\n",
    "    calculate_activation_statistics, calculate_frechet_distance\n",
    ")\n",
    "from hpvaegan_code.C3D_model import C3D\n",
    "import numpy as np\n",
    "\n",
    "dims = 256\n",
    "block_idx = C3D.BLOCK_INDEX_BY_DIM[dims]\n",
    "model = C3D(block_idx)\n",
    "model.load_state_dict(torch.load(os.path.join(os.getcwd(), \"c3d.pickle\") , weights_only = True))\n",
    "model = model.cuda().eval() \n",
    "# model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7233747-dce0-48f6-ad3a-17cbf78e72f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3D(\n",
      "  (conv1): Conv3d(3, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (pool1): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (pool2): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3a): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (conv3b): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (pool3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4a): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (conv4b): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (pool4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5a): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (conv5b): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (pool5): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
      "  (fc6): Linear(in_features=8192, out_features=4096, bias=True)\n",
      "  (fc7): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (fc8): Linear(in_features=4096, out_features=487, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27a80ce7-6907-49d9-bba4-e2a28f4e64be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "C3D                                      [64, 64, 16, 64, 64]      77,109,735\n",
       "├─Conv3d: 1-1                            [64, 64, 16, 128, 128]    (5,248)\n",
       "├─ReLU: 1-2                              [64, 64, 16, 128, 128]    --\n",
       "├─MaxPool3d: 1-3                         [64, 64, 16, 64, 64]      --\n",
       "├─Conv3d: 1-4                            [64, 128, 16, 64, 64]     (221,312)\n",
       "├─ReLU: 1-5                              [64, 128, 16, 64, 64]     --\n",
       "├─MaxPool3d: 1-6                         [64, 128, 8, 32, 32]      --\n",
       "├─Conv3d: 1-7                            [64, 256, 8, 32, 32]      (884,992)\n",
       "├─ReLU: 1-8                              [64, 256, 8, 32, 32]      --\n",
       "├─Conv3d: 1-9                            [64, 256, 8, 32, 32]      (1,769,728)\n",
       "├─ReLU: 1-10                             [64, 256, 8, 32, 32]      --\n",
       "├─MaxPool3d: 1-11                        [64, 256, 4, 16, 16]      --\n",
       "==========================================================================================\n",
       "Total params: 79,991,015\n",
       "Trainable params: 0\n",
       "Non-trainable params: 79,991,015\n",
       "Total mult-adds (T): 2.41\n",
       "==========================================================================================\n",
       "Input size (MB): 201.33\n",
       "Forward/backward pass size (MB): 15032.39\n",
       "Params size (MB): 11.53\n",
       "Estimated Total Size (MB): 15245.24\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size = [64, 3, 16, 128, 128] , device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58c3bbb4-c6bd-4ba9-a615-b5d34be594d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'VidPanos/'\n",
      "/home/jaykumaran/Vision_Projects/VidPanos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaykumaran/miniconda3/envs/torch_2x/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "cd VidPanos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5685ef9a-a6c4-43bb-85c5-70d5a11cd517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames in dir 88\n",
      "Video rendered from frames torch.Size([1, 3, 88, 1280, 720])\n"
     ]
    }
   ],
   "source": [
    "frames_path = \"data/data/real/VID_20160326_121131_02/original\"\n",
    "num_frames = int(len(os.listdir(frames_path)))\n",
    "print(\"Total frames in dir\", num_frames)\n",
    "video = read_video_from_frames(frames_path, num_frames)\n",
    "print(\"Video rendered from frames\", video.shape) #B,C,T,H,W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04dece05-3591-4b26-840f-192ff776f879",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 19.34 GiB. GPU 0 has a total capacity of 5.76 GiB of which 3.50 GiB is free. Including non-PyTorch memory, this process has 2.21 GiB memory in use. Of the allocated memory 2.11 GiB is allocated by PyTorch, and 7.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#sample code \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#https://drive.google.com/drive/folders/1T2Gff-e7jbimxioPUDq6e7Y5kQs3jwDa\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_2x/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_2x/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Vision_Projects/VidPanos/hpvaegan_code/C3D_model.py:121\u001b[0m, in \u001b[0;36mC3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_input:\n\u001b[1;32m    119\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255.0\u001b[39m \u001b[38;5;241m*\u001b[39m x  \u001b[38;5;66;03m# Scale from range (0, 1) to range (0, 255)\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    122\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(h)\n\u001b[1;32m    123\u001b[0m outp \u001b[38;5;241m=\u001b[39m [h]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_2x/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_2x/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_2x/lib/python3.10/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_2x/lib/python3.10/site-packages/torch/nn/modules/conv.py:603\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    593\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    594\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    602\u001b[0m     )\n\u001b[0;32m--> 603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 19.34 GiB. GPU 0 has a total capacity of 5.76 GiB of which 3.50 GiB is free. Including non-PyTorch memory, this process has 2.21 GiB memory in use. Of the allocated memory 2.11 GiB is allocated by PyTorch, and 7.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "pred = model(video.to(\"cuda\"))\n",
    "\n",
    "#sample code \n",
    "#https://drive.google.com/drive/folders/1T2Gff-e7jbimxioPUDq6e7Y5kQs3jwDa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ee741-4283-4c42-8228-4dac95a2f6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
